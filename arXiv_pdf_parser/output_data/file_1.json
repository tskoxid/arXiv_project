{"Introduction": "Introduction  Approaches based on deep convolutional neural networks have advanced the state-of-the-art across many standard datasets for vision problems since AlexNet . At the same time, the most prominent architecture of choice in sequence-to-sequence modelling (e.g. in natural language processing) is the transformer , which does not use convolutions, but is based on multi-headed self-attention. This operation is particularly effective at modelling long-range dependencies and allows the model to attend over all elements in the input sequence. This is in stark contrast to convolutions where the corresponding \u201creceptive field\u201d is limited, and grows linearly with the depth of the network.  The success of attention-based models in NLP has recently inspired approaches in computer vision to integrate transformers into CNNs , as well as some attempts to replace convolutions completely . However, it is  only very recently with the Vision Transformer (ViT) , that a pure-transformer based architecture has outperformed its convolutional counterparts in image classification. Dosovitskiy et al.  closely followed the original transformer architecture of , and noticed that its main benefits were observed at large scale \u2013 as transformers lack some of the inductive biases of convolutions (such as translational equivariance), they seem to require more data  or stronger regularisation .  Inspired by ViT, and the fact that attention-based architectures are an intuitive choice for modelling longrange contextual relationships in video, we develop several transformer-based models for video classification. Currently, the most performant models are based on deep 3D convolutional architectures . Recently, these models were augmented by incorporating selfattention into their later layers to better capture long-range dependencies .  As shown in Fig. 1, we propose pure-transformer models for video classification. The main operation performed in this architecture is self-attention, and it is computed on a sequence of spatio-temporal tokens that we extract from the input video. To effectively process the large number of spatio-temporal tokens that may be encountered in video, we present several methods of factorising our model along spatial and temporal dimensions to increase efficiency and scalability. Furthermore, to train our model effectively on smaller datasets, we show how to reguliarise our model during training and leverage pretrained image models.  We also note that convolutional models have been developed by the community for several years, and there are thus many \u201cbest practices\u201d associated with such models. As pure-transformer models present different characteristics, we need to determine the best design choices for such architectures. We conduct a thorough ablation analysis of tokenisation strategies, model architecture and regularisation methods. Informed by this analysis, we achieve stateof-the-art results on multiple standard video classification benchmarks, including Kinetics 400 and 600 , Epic Kitchens 100 . We will release code and models.  Figure 1: We propose a pure-transformer architecture for video classification, inspired by the recent success of such models for images . To effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components of the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different attention patterns over space and time.  2.   ", "Related_Works": "Related Work  Architectures for video understanding have mirrored advances in image recognition. Early video research used hand-crafted features to encode appearance and motion information . These models processed RGB frames and optical flow images independently before fusing them at the end. Availability of larger video classification datasets such as Kinetics  which have significantly more parameters and thus require larger training datasets. As 3D convolutional networks require significantly more computation than their image counterparts, many architectures factorise convolutions across spatial and temporal dimensions and/or use grouped convolutions . We also leverage factorisation of the spatial and temporal dimensions of videos to increase efficiency, but in the context of transformer-based models. in natural language processing (NLP), Vaswani et al.  achieved state-of-the-art results by replacing convolutions and recurrent networks with the transformer network that consisted only of self-attention, layer normalisation and multilayer perceptron (MLP) operations. Current state-of-the-art architectures in NLP . Many variants of the transformer have also been proposed to reduce the computational cost of self-attention when processing longer sequences  and to improve parameter efficiency . Although self-attention has been employed extensively in computer vision, it has, in contrast, been typically incorporated as a layer at the end or in the later stages of the network  or to augment residual  Concurrently,  blocks .  Although previous works attempted to replace convolutions in vision architectures  showed with their ViT architecture that pure-transformer networks, similar to those employed in NLP, can achieve state-of-the-art results for image classification too. The authors showed that such models are only effective at large scale, as transformers lack some of inductive biases of convolutional networks (such as translational equivariance), and thus require datasets larger than the common ImageNet ILSRVC dataset  to train. ViT has inspired a large amount of follow-up work in the community, and we note that there are a number of concurrent approaches on extending it to other tasks in computer vision  have also proposed transformer-based models for video.  In this paper, we develop pure-transformer architectures for video classification. We propose several variants of our model, including those that are more efficient by factorising the spatial and temporal dimensions of the input video. We also show how additional regularisation and pretrained models can be used to combat the fact that video datasets are not as large as their image counterparts that ViT was originally trained on. Furthermore, we outperform the stateof-the-art across five popular datasets.  3. V  ", "Main": "We start by summarising the recently proposed Vision Transformer \\ in Sec\\. 3\\.1, and then discuss two approaches for extracting tokens from video in Sec\\. 3\\.2\\. Finally, we develop several transformer-based architectures for video classification in Sec\\. 3\\.3 and 3\\.4\\.  Vision Transformer (ViT) \\ adapts the transformer architecture of \\ to process 2D images with minimal changes\\. In particular, ViT extracts N non-overlapping image patches, xi \u2208 Rh\u00d7w, performs a linear projection and then rasterises them into 1D tokens zi \u2208 Rd\\. The sequence of tokens input to the following transformer encoder is  z = \\ + p,  where the projection by E is equivalent to a 2D convolution\\. As shown in Fig\\. 1, an optional learned classification token zcls is prepended to this sequence, and its representation at the final layer of the encoder serves as the final representation used by the classification layer \\\\. In addition, a learned positional embedding, p \u2208 RN\u00d7d, is added to the tokens to retain positional information, as the subsequent self-attention operations in the transformer are permutation invariant\\. The tokens are then passed through an encoder consisting of a sequence of L transformer layers\\. Each layer  comprises of Multi-Headed Self-Attention \\, and MLP blocks as follows:  y = MSA(LN(z)) + z z+1 = MLP(LN(y)) + y\\.  The MLP consists of two linear projections separated by a GELU non-linearity \\ and the token-dimensionality, d, remains fixed throughout all layers\\. Finally, a linear classicls \u2208 Rd, fier is used to classify the encoded input based on zL if it was prepended to the input, or a global average pooling of all the tokens, zL, otherwise\\.  As the transformer \\, which forms the basis of ViT \\, is a flexible architecture that can operate on any sequence of input tokens z \u2208 RN\u00d7d, we describe strategies for tokenising videos next\\. 3\\.2\\. Embedding video clips  We consider two simple methods for mapping a video V \u2208 RT\u00d7H\u00d7W\u00d7C to a sequence of tokens \u02dcz \u2208 Rnt\u00d7nh\u00d7nw\u00d7d\\. We then add the positional embedding and reshape into RN\u00d7d to obtain z, the input to the transformer\\. Uniform frame sampling As illustrated in Fig\\. 2, a straightforward method of tokenising the input video is to uniformly sample nt frames from the input video clip, embed each 2D frame independently using the same method as ViT \\, then a total of nt\u00b7nh\u00b7nw tokens will be forwarded through the transformer encoder\\. Intuitively, this process may be seen as simply constructing a large 2D image to be tokenised following ViT\\. We note that this is the input embedding method employed by the concurrent work of \\\\.  Figure 2: Uniform frame sampling: We simply sample nt frames, and embed each 2D frame independently following ViT \\\\.  Figure 3: Tubelet embedding\\. We extract and linearly embed nonoverlapping tubelets that span the spatio-temporal input volume\\.  t , nh =  H  Tubelet embedding An alternate method, as shown in Fig\\. 3, is to extract non-overlapping, spatio-temporal \u201ctubes\u201d from the input volume, and to linearly project this to Rd\\. This method is an extension of ViT\u2019s embedding to 3D, and corresponds to a 3D convolution\\. For a tubelet of dimension t \u00d7 h \u00d7 w, nt =  T w , h  and nw =  W tokens are extracted from the temporal, height, and width dimensions respectively\\. Smaller tubelet dimensions thus result in more tokens which increases the computation\\. Intuitively, this method fuses spatio-temporal information during tokenisation, in contrast to \u201cUniform frame sampling\u201d where temporal information from different frames is fused by the transformer\\. 3\\.3\\. Transformer Models for Video  As illustrated in Fig\\. 1, we propose multiple transformerbased architectures\\. We begin with a straightforward extension of ViT \\ that models pairwise interactions between all spatio-temporal tokens, and then develop more efficient variants which factorise the spatial and temporal dimensions of the input video at various levels of the transformer architecture\\.  Model 1: Spatio-temporal attention This model simply forwards all spatio-temporal tokens extracted from the video, z0, through the transformer encoder\\. We note that this has also been explored concurrently by \\ in their \u201cJoint Space-Time\u201d model\\. In contrast to CNN architectures, where the receptive field grows linearly with the number of layers, each transformer layer models all pair #!\"!\"#Figure 4: Factorised encoder (Model 2)\\. This model consists of two transformer encoders in series: the first models interactions between tokens extracted from the same temporal index to produce a latent representation per time-index\\. The second transformer models interactions between time steps\\. It thus corresponds to a \u201clate fusion\u201d of spatial- and temporal information\\.  wise interactions between all spatio-temporal tokens, and it thus models long-range interactions across the video from the first layer\\. However, as it models all pairwise interactions, Multi-Headed Self Attention (MSA) \\ has quadratic complexity with respect to the number of tokens\\. This complexity is pertinent for video, as the number of tokens increases linearly with the number of input frames, and motivates the development of more efficient architectures next\\.  Model 2: Factorised encoder As shown in Fig\\. 4, this model consists of two separate transformer encoders\\. The first, spatial encoder, only models interactions between tokens extracted from the same temporal index\\. A representation for each temporal index, hi \u2208 Rd, is obtained after Ls layers: This is the encoded classification token, zLs cls if it was prepended to the input (Eq\\. 1), or a global average pooling from the tokens output by the spatial encoder, zLs, otherwise\\. The frame-level representations, hi, are concatenated into H \u2208 Rnt\u00d7d, and then forwarded through a temporal encoder consisting of Lt transformer layers to model interactions between tokens from different temporal indices\\. The output token of this encoder is then finally classified\\.  This architecture corresponds to a \u201clate fusion\u201d \\ of temporal information, and the initial spatial encoder is identical to the one used for image classiIt is thus analogous to CNN architectures such fication\\. as \\ which first extract per-frame features, and then aggregate them into a final representation before classifying them\\. Although this model has more transformer layers than Model 1 (and thus more parameters), it requires fewer floating point operations (FLOPs), as the two separate transformer blocks have a complexity of O((nh \u00b7 nw)2 + n2 t ) compared to O((nt \u00b7 nh \u00b7 nw)2) of Model 1\\.  Figure 5: Factorised self-attention (Model 3)\\. Within each transformer block, the multi-headed self-attention operation is factorised into two operations (indicated by striped boxes) that first only compute self-attention spatially, and then temporally\\. Model 3: Factorised self-attention This model, in contrast, contains the same number of transformer layers as Model 1\\. However, instead of computing multi-headed self-attention across all pairs of tokens, z, at layer l, we factorise the operation to first only compute self-attention spatially (among all tokens extracted from the same temporal index), and then temporally (among all tokens extracted from the same spatial index) as shown in Fig\\. 5\\. Each self-attention block in the transformer thus models spatio-temporal interactions, but does so more efficiently than Model 1 by factorising the operation over two smaller sets of elements, thus achieving the same computational complexity as Model 2\\. We note that factorising attention over input dimensions has also been explored in \\, and concurrently in the context of video by \\ in their \u201cDivided Space-Time\u201d model\\. This operation can be performed efficiently by reshaping the tokens z from R1\u00d7nt\u00b7nh\u00b7nw\u00b7d to Rnt\u00d7nh\u00b7nw\u00b7d (denoted by zs) to compute spatial self-attention\\. Similarly, the input to temporal self-attention, zt is reshaped to Rnh\u00b7nw\u00d7nt\u00b7d\\. Here we assume the leading dimension is the \u201cbatch dimension\u201d\\. Our factorised self-attention is defined as  y s = MSA(LN(z t = MSA(LN(y y z+1 = MLP(LN(y  s)) + z s s)) + y s t)) + y t\\.  We observed that the order of spatial-then-temporal selfattention or temporal-then-spatial self-attention does not make a difference, provided that the model parameters are initialised as described in Sec\\. 3\\.4\\. Note that the number of parameters, however, increases compared to Model 1, as there is an additional self-attention layer (cf\\. Eq\\. 7)\\. We do not use a classification token in this model, to avoid ambiguities when reshaping the input tokens between spatial and temporal dimensions\\. Model 4: Factorised dot-product attention Finally, we develop a model which has the same computational complexity as Models 2 and 3, while retaining the same number of parameters as the unfactorised Model 1\\. The factorisation of spatial- and temporal dimensions is similar in spirit  E =  1 t  We consider an additional strategy, which we denote as \u201ccentral frame initialisation\u201d, where E is initialised with zeroes along all temporal positions, except at the centre  t 2, (9)  E = \\\\.  Therefore, the 3D convolutional filter effectively behaves like \u201cUniform frame sampling\u201d (Sec\\. 3\\.2) at initialisation, while also enabling the model to learn to aggregate temporal information from multiple frames as training progresses\\. Transformer weights for Model 3 The transformer block in Model 3 (Fig\\. 5) differs from the pretrained ViT model \\, in that it contains two multi-headed self attention (MSA) modules\\. In this case, we initialise the spatial MSA module from the pretrained module, and initialise all weights of the temporal MSA with zeroes, such that Eq\\. 5 behaves as a residual connection \\ at initialisation\\.  4\\. Empirical evaluation  We first present our experimental setup and implementation details in Sec\\. 4\\.1, before ablating various components of our model in Sec\\. 4\\.2\\. We then present state-of-the-art results on five datasets in Sec\\. 4\\.3\\. 4\\.1\\. Experimental Setup Network architecture and training Our backbone architecture follows that of ViT \\\\. We consider ViT-Base (ViT-B, L=12, NH=12, d=3072), ViT-Large (ViT-L, L=24, NH=16, d=4096), and ViT-Huge (ViT-H, L=32, NH=16, d=5120), where L is the number of transformer layers, each with a self-attention block of NH heads  Figure 6: Factorised dot-product attention (Model 4)\\. For half of the heads, we compute dot-product attention over only the spatial axes, and for the other half, over only the temporal axis\\.  to Model 3, but we factorise the multi-head dot-product attention operation instead (Fig\\. 6)\\. Concretely, we compute attention weights for each token separately over the spatialand temporal-dimensions using different heads\\. First, we note that the attention operation for each head is defined as    V\\.  dk  Attention(Q, K, V) = Softmax  In self-attention, the queries Q = XWq, keys K = XWk, and values V = XWv are linear projections of the input X with X, Q, K, V \u2208 RN\u00d7d\\. Note that in the unfactorised case (Model 1), the spatial and temporal dimensions are merged as N = nt \u00b7 nh \u00b7 nw\\.  The main idea here is to modify the keys and values for each query to only attend over tokens from the same spatialand temporal index by constructing Ks, Vs \u2208 Rnh\u00b7nw\u00d7d and Kt, Vt \u2208 Rnt\u00d7d, namely the keys and values corresponding to these dimensions\\. Then, for half of the attention heads, we attend over tokens from the spatial dimension by computing Ys = Attention(Q, Ks, Vs), and for the rest we attend over the temporal dimension by computing Yt = Attention(Q, Kt, Vt)\\. Given that we are only changing the attention neighbourhood for each query, the attention operation has the same dimension as in the unfactorised case, namely Ys, Yt \u2208 RN\u00d7d\\. We then combine the outputs of multiple heads by concatenating them and using a linear projection \\, Y = Concat(Ys, Yt)WO\\.  3\\.4\\. Initialisation by leveraging pretrained models ViT \\ has been shown to only be effective when trained on large-scale datasets, as transformers lack some of the inductive biases of convolutional networks \\, have several orders of magnitude less labelled examples when compared to their image counterparts \\\\. As a result, training large models from scratch to high accuracy is extremely challenging\\. To sidestep this issue, and enable more efficient training we initialise our video models from pretrained image models\\. However, this raises several practical questions, specifically on how to initialise parameters not present or incompatible with image models\\. We now  K         V       Q Self-Attention BlockLayer NormMulti-HeadDot-product AttentionConcatenateLinearK             V                Q Scaled Dot-Product AttentionLinearLinearLinearSpatial HeadsK             V                Q Scaled Dot-Product AttentionLinearLinearLinearTemporal HeadsTable 1: Comparison of input encoding methods using ViViT-B and spatio-temporal attention on Kinetics\\. Further details in text\\.  Top-1 accuracy  Uniform frame sampling Tubelet embedding Random initialisation \\ Filter inflation \\ Central frame  78\\.5  73\\.2 77\\.6 79\\.2  and hidden dimension d\\. We also apply the same naming scheme to our models (e\\.g\\., ViViT-B/16x2 denotes a ViTBase backbone with a tubelet size of h\u00d7w\u00d7t = 16\u00d716\u00d72)\\. In all experiments, the tubelet height and width are equal\\. Note that smaller tubelet sizes correspond to more tokens at the input, and thus more computation\\.  We train our models using synchronous SGD and momentum, a cosine learning rate schedule and TPU-v3 accelerators\\. We initialise our models from a ViT image model trained either on ImageNet-21K \\ dataset\\. Exact experimental hyperparameters are detailed in the appendix\\.  Datasets We evaluate the performance of our proposed models on a diverse set of video classification datasets:  Kinetics \\ consists of 10-second videos sampled at 25fps from YouTube\\. We evaluate on both Kinetics 400 and 600, containing 400 and 600 classes respectively\\. As these are dynamic datasets (videos may be removed from YouTube), we note our dataset sizes are approximately 267 000 and 446 000 respectively\\.  Epic Kitchens-100 consists of egocentric videos capturing daily kitchen activities spanning 100 hours and 90 000 clips \\\\. We report results following the standard \u201caction recognition\u201d protocol\\. Here, each video is labelled with a \u201cverb\u201d and a \u201cnoun\u201d and we therefore predict both categories using a single network with two \u201cheads\u201d\\. The topscoring verb and action pair predicted by the network form an \u201caction\u201d, and action accuracy is the primary metric\\.  Moments in Time \\ consists of 800 000, 3-second YouTube clips that capture the gist of a dynamic scene involving animals, objects, people, or natural phenomena\\.  Something-Something v2 (SSv2) \\ contains 220 000 videos, with durations ranging from 2 to 6 seconds\\. In contrast to the other datasets, the objects and backgrounds in the videos are consistent across different action classes, and this dataset thus places more emphasis on a model\u2019s ability to recognise fine-grained motion cues\\.  Inference The input to our network is a video clip of 32 frames using a stride of 2, unless otherwise mentioned, similar to \\\\. Following common practice, at inference time, we process multiple views of a longer video and average per-view logits to obtain the final result\\. Unless other Table 2: Comparison of model architectures using ViViT-B as the backbone, and tubelet size of 16\u00d7 2\\. We report Top-1 accuracy on Kinetics 400 (K400) and action accuracy on Epic Kitchens (EK)\\. Runtime is during inference on a TPU-v3\\.  K400  EK  Model 1: Spatio-temporal Model 2: Fact\\. encoder Model 3: Fact\\. self-attention Model 4: Fact\\. dot product Model 2: Ave\\. pool baseline  80\\.0 78\\.8 77\\.4 76\\.3 75\\.8  43\\.1 43\\.7 39\\.1 39\\.5 38\\.8  FLOPs (\u00d7109) 455\\.2 284\\.4 372\\.3 277\\.1 283\\.9  Params (\u00d7106) 88\\.9 100\\.7 117\\.3 88\\.9 86\\.7  Runtime  Table 3: The effect of varying the number of temporal transformers, Lt, in the Factorised encoder model (Model 2)\\. We report the Top-1 accuracy on Kinetics 400\\. Note that Lt = 0 corresponds to the \u201caverage pooling baseline\u201d\\. 0  8  Lt Top-1  75\\.8  78\\.6  78\\.8  78\\.8  12 78\\.9  wise specified, we use a total of 4 views per video (as this is sufficient to \u201csee\u201d the entire video clip across the various datasets), and ablate these and other design choices next\\. 4\\.2\\. Ablation study Input encoding We first consider the effect of different input encoding methods (Sec\\. 3\\.2) using our unfactorised model (Model 1) and ViViT-B on Kinetics 400\\. As we pass 32-frame inputs to the network, sampling 8 frames and extracting tubelets of length t = 4 correspond to the same number of tokens in both cases\\. Table 1 shows that tubelet embedding initialised using the \u201ccentral frame\u201d method (Eq\\. 9) performs well, outperforming the commonly-used \u201cfilter inflation\u201d initialisation method \\ by 1\\.6%, and \u201cuniform frame sampling\u201d by 0\\.7%\\. We therefore use this encoding method for all subsequent experiments\\. Model variants We compare our proposed model variants (Sec\\. 3\\.3) across the Kinetics 400 and Epic Kitchens datasets, both in terms of accuracy and efficiency, in Tab\\. 2\\. In all cases, we use the \u201cBase\u201d backbone and tubelet size of 16 \u00d7 2\\. Model 2 (\u201cFactorised Encoder\u201d) has an additional hyperparameter, the number of temporal transformers, Lt\\. We set Lt = 4 for all experiments and show in Tab\\. 3 that the model is not sensitive to this choice\\.  The unfactorised model (Model 1) performs the best on Kinetics 400\\. However, it can also overfit on smaller datasets such as Epic Kitchens, where we find our \u201cFactorised Encoder\u201d (Model 2) to perform the best\\. We also consider an additional baseline (last row), based on Model 2, where we do not use any temporal transformer, and simply average pool the frame-level representations from the spatial encoder before classifying\\. This average pooling baseline performs the worst, and has a larger accuracy drop on Epic Kitchens, suggesting that this dataset requires more detailed modelling of temporal relations\\.  Table 4: The effect of progressively adding regularisation (each row includes all methods above it) on Top-1 action accuracy on Epic Kitchens\\. We use a Factorised encoder model with tubelet size 16 \u00d7 2\\.  Top-1 accuracy  Random crop, flip, colour jitter + Kinetics 400 initialisation + Stochastic depth \\ + Random augment \\ + Label smoothing \\ + Mixup \\  38\\.4 39\\.6 40\\.2 41\\.1 43\\.1 43\\.7  Figure 7: The effect of the backbone architecture on (a) accuracy and (b) computation on Kinetics 400, for the spatio-temporal attention model (Model 1)\\.  Figure 8: The effect of varying the number of temporal tokens on (a) accuracy and (b) computation on Kinetics 400, for different variants of our model with a ViViT-B backbone\\.  As described in Sec\\. 3\\.3, all factorised variants of our model use significantly fewer FLOPs than the unfactorised Model 1, as the attention is computed separately over spatial- and temporal-dimensions\\. Model 4 adds no additional parameters to the unfactorised Model 1, and uses the least compute\\. The temporal transformer encoder in Model 2 operates on only nt tokens, which is why there is a barely a change in compute and runtime over the average pooling baseline, even though it improves the accuracy substantially (3% on Kinetics and 4\\.9% on Epic Kitchens)\\. Finally, Model 3 requires more compute and parameters than the other factorised models, as its additional self-attention block means that it performs another query-, key-, valueand output-projection in each transformer layer \\\\. Model architectures such as ViT \\ are known to require large training datasets, and we observed overfitting on smaller datasets like Epic Kitchens and SSv2, even when using an ImageNet pretrained model\\. In order to effectively train our models on such datasets, we employed several regularisation strategies that we ablate using our \u201cFactorised encoder\u201d model  regularisation Pure-transformer  Table 5: The effect of spatial resolution on the performance of ViViT-L/16x2 and spatio-temporal attention on Kinetics 400\\.  Crop size Accuracy GFLOPs Runtime  224 80\\.3 1446 58\\.9  288 80\\.7 2919 147\\.6  320 81\\.0 3992 238\\.8  in Tab\\. 4\\. We note that these regularisers were originally proposed for training CNNs, and that \\ have recently explored them for training ViT for image classification\\.  Each row of Tab\\. 4 includes all the methods from the rows above it, and we observe progressive improvements from adding each regulariser\\. Overall, we obtain a substantial overall improvement of 5\\.3% on Epic Kitchens\\. We also achieve a similar improvement of 5%, from 60\\.4% to 65\\.4%, on SSv2 by using all the regularisation in Tab\\. 4\\. Note that the Kinetics-pretrained models that we initialise from are from Tab\\. 2, and that all Epic Kitchens models in Tab\\. 2 were trained with all the regularisers in Tab\\. 4\\. For larger datasets like Kinetics and Moments in Time, we do not use these additional regularisers (we use only the first row of Tab\\. 4), as we obtain state-of-the-art results without them\\. The appendix contains hyperparameter values and additional details for all regularisers\\.  Varying the backbone Figure 7 compares the ViViTB and ViViT-L backbones for the unfactorised spatiotemporal model\\. We observe consistent improvements in accuracy as the backbone capacity increases\\. As expected, the compute also grows as a function of the backbone size\\.  Varying the number of tokens We first analyse the performance as a function of the number of tokens along the temporal dimension in Fig\\. 8\\. We observe that using smaller input tubelet sizes (and therefore more tokens) leads to consistent accuracy improvements across all of our model architectures\\. At the same time, computation in terms of FLOPs increases accordingly, and the unfactorised model (Model 1) is impacted the most\\.  We then vary the number of tokens fed into the model by increasing the spatial crop-size from the default of 224 to 320 in Tab\\. 5\\. As expected, there is a consistent increase in both accuracy and computation\\. We note that when comparing to prior work we consistently obtain state-of-the-art results (Sec\\. 4\\.3) using a spatial resolution of 224, but we also highlight that further improvements can be obtained at higher spatial resolutions\\.  Varying the number of input frames In our experiments so far, we have kept the number of input frames fixed to 32\\. We now ablate this choice whilst effectively keeping the amount of computation to process a single clip fixed\\. This is done by increasing the tubelet length t in proportion to  16x816x416x2Input tubelet size787980Top-1 Accuracy16x816x416x2Input tubelet size0\\.51\\.01\\.5TFLOPsViViT-BViViT-L16x816x416x2Input tubelet size72\\.575\\.077\\.580\\.0Top-1 Accuracy16x816x416x2Input tubelet size0\\.20\\.4TFLOPsSpatio-temporalFactorised encoderFactorised self-attentionFactorised dot-productTable 6: Comparisons to state-of-the-art across multiple datasets\\. For \u201cviews\u201d, x \u00d7 y denotes x temporal crops and y spatial crops\\. \u201c320\u201d denotes models trained and tested with a spatial resolution of 320 instead of 224\\.  Method  Top 1 Top 5  Views  Method  Top 1 Top 5  Views  Method  Action Verb Noun  blVNet \\ STM \\ TEA \\ TSM-ResNeXt-101 \\ I3D NL \\ CorrNet-101 \\ ip-CSN-152 \\ LGD-3D R101 \\ SlowFast R101-NL \\ X3D-XXL \\ TimeSformer-L \\ ViViT-L/16x2 ViViT-L/16x2 320  73\\.5 73\\.7 76\\.1 76\\.3 77\\.7 79\\.2 79\\.2 79\\.4 79\\.8 80\\.4 80\\.7 80\\.6 81\\.3  Methods with large-scale pretraining ip-CSN-152 \\) ViViT-L/16x2 (JFT) ViViT-L/16x2 320 (JFT) ViViT-H/16x2 (JFT)  82\\.5 82\\.8 83\\.5 84\\.8  91\\.2 91\\.6 92\\.5  \u2013  93\\.3  \u2013  93\\.8 94\\.4 93\\.9 94\\.6 94\\.7 94\\.7 94\\.7  95\\.3 95\\.5 95\\.5 95\\.8  \u2013  10 \u00d7 3 10 \u00d7 3 10 \u00d7 3 10 \u00d7 3 10 \u00d7 3 10 \u00d7 3 1 \u00d7 3 4 \u00d7 3 4 \u00d7 3  \u2013  10 \u00d7 3 4 \u00d7 3 4 \u00d7 3 4 \u00d7 3  AttentionNAS \\ LGD-3D R101 \\ SlowFast R101-NL \\ X3D-XL \\ TimeSformer-HR \\ ViViT-L/16x2 ViViT-L/16x2 320  ViViT-L/16x2 (JFT) ViViT-H/16x2 (JFT)  79\\.8 81\\.5 81\\.8 81\\.9 82\\.4 82\\.5 83\\.0  84\\.3 85\\.8  94\\.4 95\\.6 95\\.1 95\\.5 96\\.0 95\\.6 95\\.7  96\\.2 96\\.5  \u2013  10 \u00d7 3 10 \u00d7 3 4 \u00d7 3 4 \u00d7 3 4 \u00d7 3 4 \u00d7 3  TSN \\ TRN \\ TBN \\ TSM \\ SlowFast \\  ViViT-L/16x2 Fact\\. encoder  33\\.2 35\\.3 36\\.7 38\\.3 38\\.5 44\\.0  60\\.2 65\\.9 66\\.0 67\\.9 65\\.6  66\\.4  46\\.0 45\\.4 47\\.2 49\\.0 50\\.0 56\\.8  Method  Top 1  Top 5  TSN \\ TRN \\ I3D \\ blVNet \\ AssembleNet-101 \\  ViViT-L/16x2  25\\.3 28\\.3 29\\.5 31\\.4 34\\.3 38\\.0  Top 5  50\\.1 53\\.4 56\\.1 59\\.3 62\\.7 64\\.9  TRN \\ SlowFast \\ TimeSformer-HR \\ TSM \\ STM \\ TEA \\ blVNet \\  ViViT-L/16x2 Fact\\. encoder  48\\.8 61\\.7 62\\.5 63\\.4 64\\.2 65\\.1 65\\.2 65\\.4  77\\.6  88\\.5 89\\.8  89\\.8  process longer videos without increasing the number of tokens, they offer an efficient method for processing longer videos than those considered by existing video classification datasets, and keep it as an avenue for future work\\.  4\\.3\\. Comparison to state-of-the-art  Based on our ablation studies in the previous section, we compare to the current state-of-the-art using two of our model variants\\. We use the unfactorised spatio-temporal attention model (Model 1) for the larger datasets, Kinetics and Moments in Time\\. For the smaller datasets, Epic Kitchens and SSv2, we use our Factorised encoder model (Model 2)\\.  Kinetics Tables 6a and 6b show that our spatio-temporal attention models outperform the state-of-the-art on Kinetics 400 and 600 respectively\\. Following standard practice, we take 3 spatial crops (left, centre and right) \\ for each temporal view, and notably, we require significantly fewer views than previous CNN-based methods\\.  We surpass the previous CNN-based state-of-the-art using ViViT-L/16x2 pretrained on ImageNet, and achieve comparable accuracy to \\ who concurrently proposed a pure-transformer architecture\\. We then obtain further improvements by increasing the spatial resolution from 224 to 320 as expected given the ablation in Tab\\. 5\\. Moreover, by initialising our backbones from models pretrained on the larger JFT dataset \\, we obtain further improvements\\. Although these models are not directly comparable to previous work, we do also outperform \\ who pretrained on the large-scale, Instagram dataset \\\\. Our best model uses a ViViT-H backbone pretrained on JFT and significantly advances the best reported results on Kinetics 400 and 600 to 84\\.8% and 85\\.8%, respectively\\.  Figure 9: The effect of varying the number of frames input to the network when keeping the number of tokens constant by adjusting the tubelet length t\\. We use ViViT-B, and spatio-temporal attention on Kinetics 400\\. A Kinetics video contains 250 frames (10 seconds sampled at 25 fps) and the accuracy for each model saturates once the number of equidistant temporal views is sufficient to \u201csee\u201d the whole video clip\\.  the number of input frames, such that the number of tokens processed by the network is constant\\.  Figure 9 shows that as we increase the number of frames input to the network, and enlarge the tubelet length, t, accordingly, our accuracy from processing a single clip increases, as the network incorporates longer temporal context\\. However, common practice on datasets such as Kinetics \\ is to average results over multiple, shorter \u201cviews\u201d of the same video clip\\. Following this multi-view testing protocol, processing shorter clips is actually more advantageous\\. We also observe from Fig\\. 9 that the accuracy saturates once the number of views is sufficent to cover the whole video, and that this is consistent across models with different numbers of input frames\\. Consequently, we use 32 frames, sampled with a stride of 2, as our networkinput for all experiments, and use 4 temporal views for multi-view testing\\. Finally, we note that as our models can  1234567Number of views7678Top-1 Accuracy32 stride 264 stride 2128 stride 2Moments in Time We surpass the state-of-the-art by a significant margin as shown in Tab\\. 6c\\. We note that the videos in this dataset are diverse and contain significant label noise, making this task challenging and leading to lower accuracies than on other datasets\\.  Epic Kitchens 100 Table 6d shows that our Factorised encoder model outperforms previous methods by a significant margin\\. In addition, our model obtains substantial improvements for Top-1 accuracy of \u201cnoun\u201d classes, and the only method which achieves higher \u201cverb\u201d accuracy used optical flow as an additional input modality \\\\. Furthermore, all variants of our model presented in Tab\\. 2 outperformed the existing state-of-the-art on action accuracy\\. We note that we use the same model to predict verbs and nouns using two separate \u201cheads\u201d, and for simplicity, we do not use separate loss weights for each head\\.  Something-Something v2 (SSv2) Finally, Tab\\. 6e shows that we achieve state-of-the-art Top-1 accuracy with our Factorised encoder model (Model 2), albeit with a smaller margin compared to previous methods\\. Notably, our Factorised encoder model significantly outperforms the concurrent TimeSformer \\ method by 2\\.9%, which also proposes a pure-transformer model, but does not consider our Factorised encoder variant or our additional regularisation\\.  SSv2 differs from other datasets in that the backgrounds and objects are quite similar across different classes, meaning that recognising fine-grained motion patterns is necessary to distinguish classes from each other\\. Our results suggest that capturing these fine-grained motions is an area of improvement and future work for our model\\. We also note an inverse correlation between the relative performance of previous methods on SSv2 (Tab\\. 6e) and Kinetics (Tab\\. 6a) suggesting that these two datasets evaluate complementary characteristics of a model\\.  5\\. Conclusion  ", "Conclusion": "Conclusion and Future Work  We have presented four pure-transformer models for video classification, with different accuracy and efficiency profiles, achieving state-of-the-art results across five popular datasets\\. Furthermore, we have shown how to effectively regularise such high-capacity models for training on smaller datasets and thoroughly ablated our main design choices\\. Future work is to remove our dependence on image-pretrained models\\. Finally, going beyond video classification towards more complex tasks is a clear next step\\.   "}