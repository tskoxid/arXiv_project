Extremely Small BERT Models from Mixed-Vocabulary Training

Sanqiang Zhao*1, Raghav Gupta*2, Yang Song3, and Denny Zhou4

1University of Pittsburgh, Pittsburgh, PA sanqiang.zhao@pitt.edu
2Google Research, Mountain View, CA raghavgupta@google.com
3Kuaishou Technology, Beijing, China
yangsong@kuaishou.com

4Google Brain, Mountain View, CA dennyzhou@google.com

Abstract

Pretrained language models like BERT have
achieved good results on NLP tasks, but are
impractical on resource-limited devices due to
memory footprint. A large fraction of this foot-
print comes from the input embeddings with
large input vocabulary and embedding dimen-
sions. Existing knowledge distillation meth-
ods used for model compression cannot be di-
rectly applied to train student models with re-
duced vocabulary sizes. To this end, we pro-
pose a distillation method to align the teacher
and student embeddings via mixed-vocabulary
training. Our method compresses BERTLARGE
to a task-agnostic model with smaller vocab-
ulary and hidden dimensions, which is an or-
der of magnitude smaller than other distilled
BERT models and offers a better size-accuracy
trade-off on language understanding bench-
marks as well as a practical dialogue task.

1

Introduction

Recently, pre-trained context-aware language mod-
els like ELMo (Peters et al., 2018), GPT (Radford
et al., 2019), BERT (Devlin et al., 2018) and XLNet
(Yang et al., 2019) have outperformed traditional
word embedding models like Word2Vec (Mikolov
et al., 2013) and GloVe (Pennington et al., 2014),
and achieved strong results on a number of lan-
guage understanding tasks. However, these models
are typically too huge to host on mobile/edge de-
vices, especially for real-time inference. Recent
work has explored, inter alia, knowledge distilla-
tion (Ba and Caruana, 2014; Hinton et al., 2015)
to train small-footprint student models by implicit
transfer of knowledge from a teacher model.

Most distillation methods, however, need the stu-
dent and teacher output spaces to be aligned. This
complicates task-agnostic distillation of BERT to

Asterisk (*) denotes equal contribution. Research con-

ducted when all authors were at Google.

smaller-vocabulary student BERT models since the
input vocabulary is also the output space for the
masked language modeling (MLM) task used in
BERT. This in turn limits these distillation meth-
ods’ ability to compress the input embedding ma-
trix, that makes up a major proportion of model
parameters e.g. the ∼30K input WordPiece embed-
dings of the BERTBASE model make up over 21%
of the model size. This proportion is even higher
for most distilled BERT models, owing to these
distilled models typically having fewer layers than
their teacher BERT counterparts.

We present a task and model-agnostic distillation
approach for training small, reduced-vocabulary
BERT models running into a few megabytes. In
our setup, the teacher and student models have
incompatible vocabularies and tokenizations for
the same sequence. We therefore align the stu-
dent and teacher WordPiece embeddings by train-
ing the teacher on the MLM task with a mix of
teacher-tokenized and student-tokenized words in
a sequence, and then using these student embed-
dings to train smaller student models. Using our
method, we train compact 6 and 12-layer reduced-
vocabulary student models which achieve competi-
tive performance in addition to high compression
for benchmark datasets as well as a real-world ap-
plication in language understanding for dialogue.

2 Related Work

Work in NLP model compression falls broadly into
four classes: matrix approximation, weight quanti-
zation, pruning/sharing, and knowledge distillation.
The former two seek to map model parameters
to low-rank approximations (Tulloch and Jia, 2017)
and lower-precision integers/ﬂoats (Chen et al.,
2015; Zhou et al., 2018; Shen et al., 2019) respec-
tively. In contrast, pruning aims to remove/share
redundant model weights (Li et al., 2016; Lan et al.,
2019). More recently, dropout (Srivastava et al.,
2014) has been used to cut inference latency by

1
2
0
2

 

b
e
F
6

 

 
 
]
L
C
.
s
c
[
 
 

2
v
7
8
6
1
1

.

9
0
9
1
:
v
i
X
r
a

Figure 1: Depiction of our mixed-vocabulary training approach. (Left) Stage I involving retrained teacher BERT
with default conﬁg (e.g., 30K vocabulary, 768 hidden dim) and mixed-vocabulary input. (Right) Stage II involving
student model with smaller vocabulary (5K) and hidden dims (e.g., 256) and embeddings initialized from stage I.

early exit (Fan et al., 2019; Xin et al., 2020).

Knowledge distillation focuses on implicit trans-
fer of knowledge as soft teacher predictions (Tang
et al., 2019), attention distributions (Zagoruyko
and Komodakis, 2016) and intermediate outputs
(Romero et al., 2014). Approaches close to our
work rely on similar methods (Sanh et al., 2019;
Sun et al., 2019), while others involve combina-
tions of layer-wise transfer (Sun et al., 2020), task-
speciﬁc distillation (Jiao et al., 2019), architecture
search (Chen et al., 2020) and layer dropout (Xu
et al., 2020); many of these are speciﬁc to the trans-
former layer (Vaswani et al., 2017).

Another highly relevant line of work focuses
on reducing the size of the embedding matrix, ei-
ther via factorization (Shu and Nakayama, 2018;
Lan et al., 2019) or vocabulary selection/pruning
(Provilkov et al., 2019; Chen et al., 2019b).

3 Proposed Approach
Here, we discuss our rationale behind reducing the
student vocabulary size and its challenges, followed
by our mixed-vocabulary distillation approach.

3.1 Student Vocabulary
WordPiece (WP) tokens (Wu et al., 2016) are sub-
word units obtained by applying greedy segmenta-
tion to a training corpus. Given such a corpus and
a number of desired tokens D, a WordPiece vocab-
ulary is generated by selecting D subword tokens
such that the resulting corpus is minimal in the num-
ber of WordPiece when segmented according to the
chosen WordPiece model. The greedy algorithm
for this optimization problem is described in more

detail in Sennrich et al. (2016). Most published
BERT models use a vocabulary of 30522 Word-
Pieces, obtained by running the above algorithm on
the Wikipedia and BooksCorpus (Zhu et al., 2015)
corpora with a desired vocabulary size D of 30000.
For our student model, we chose a target vocab-
ulary size D of 5000 WordPiece tokens. Using the
same WordPiece vocabulary generation algorithm
and corpus as above, we obtain a 4928-WordPiece
vocabulary for the student model. This student vo-
cabulary includes all ASCII characters as separate
tokens, ensuring no out-of-vocabulary words upon
tokenization with this vocabulary. Additionally, the
30K teacher BERT vocabulary includes 93.9% of
the WP tokens in this 5K student vocabulary but
does not subsume it. We explore other strategies to
obtain a small student vocabulary in Section 6.

For task-agnostic student models, we reuse
BERT’s masked language modeling (MLM) task:
words in context are randomly masked and pre-
dicted given the context via softmax over the
model’s WP vocabulary. Thus, the output spaces
for our teacher (30K) and student (5K) models are
unaligned. This, coupled with both vocabularies
tokenizing the same words differently, means exist-
ing distillation methods do not apply to our setting.

3.2 Mixed-vocabulary training
We propose a two-stage approach for implicit trans-
fer of knowledge to the student via the student
embeddings, as described below.
Stage I (Student Embedding Initialization): We
ﬁrst train the student embeddings with the teacher

model initialized from BERTLARGE. For a given in-
put sequence, we mix the vocabularies by randomly
selecting (with probability pSV , a hyperparameter)
words from the sequence to segment using the stu-
dent vocabulary, with the other words segmented
using the teacher vocabulary. As in Figure 1 on
the left, for input [‘I’, ‘like’, ‘machine’, ‘learning’],
the words ‘like’ and ‘learning’ are segmented using
the student vocabulary (in blue), with the others
using the teacher vocabulary (in green). Similar
to Lample and Conneau (2019), this step seeks to
align the student and teacher embeddings for the
same tokens: the model learns to predict student
tokens using context which is segmented using the
teacher vocabulary, and vice versa.

Note that since the student embeddings are set
to a lower dimension than the teacher embeddings,
as they are meant to be used in the smaller student
model, we project the student embeddings up to
the teacher embedding dimension using a trainable
afﬁne layer before these are input to the teacher
BERT. We choose to keep the two embedding ma-
trices separate despite the high token overlap: this
is partly to keep our approach robust to lower vo-
cabulary overlap settings, and partly due to empiri-
cal considerations described in Section 6.

Let θs/ebs and θt/ebt denote the transformer
layer and embedding weights for the student and
teacher models respectively. The loss deﬁned in
Equation 1 is the MLM cross entropy summed over
masked positions Mt in the teacher input. yi and
ci denote the predicted and true tokens at position
i respectively and can belong to either vocabulary.
vi∈{s,t} denotes the vocabulary used to segment
this token. Separate softmax layers Pvi are used
for token prediction, one for each vocabulary, de-
pending on the segmenting vocabulary vi for token
i. All teacher parameters (θt, ebt) and student em-
beddings (ebs) are updated in this step.

(logPvi(yi=ci|θt, ebs, ebt)) (1)

i∈Mt

Ls1 = −(cid:80)

Stage II (Student Model Layers): With student
embeddings initialized in stage I, we now train the
student model normally i.e., using only the student
vocabulary and discarding the teacher model. Equa-
tion 2 shows the student MLM loss where Ms is
the set of positions masked in the student input. All
student model parameters (θs, ebs) are updated.

logPs(yi=ci|θs, ebs))

(2)

i∈Ms

Ls2 = −(cid:80)

4 Experiments

For evaluation, we ﬁnetune the student model just
as one would ﬁnetune the original BERT model
i.e., without using the teacher model or any task-
speciﬁc distillation. We describe our experiments
below, with dataset details left to the appendix.

4.1 Evaluation Tasks and Datasets
We ﬁne-tune and evaluate the distilled student mod-
els on two classes of language understanding tasks:
GLUE benchmark (Wang et al., 2019): We pick
three classiﬁcation tasks from GLUE:
• MRPC: Microsoft Research Paraphrase Corpus
(Dolan and Brockett, 2005), a 2-way sentence
pair classiﬁcation task with 3.7K train instances.

• MNLI: Multi-Genre Natural Language Inference
(Williams et al., 2018), a 3-way sentence pair
classiﬁcation task with 393K training instances.

• SST-2: Stanford Sentiment Treebank (Socher
et al., 2013), a 2-way sentence classiﬁcation task
with 67K training instances.

Spoken Language Understanding: Since we are
also keen on edge device applications, we also eval-
uate on spoken language understanding, a practi-
cal task in dialogue systems. We use the SNIPS
dataset (Coucke et al., 2018) of ∼14K virtual as-
sistant queries, each comprising one of 7 intents
and values for one or more of the 39 pre-deﬁned
slots. The intent detection and slot ﬁlling subtasks
are modeled respectively as 7-way sentence classi-
ﬁcation and sequence tagging with IOB slot labels.

4.2 Models and Baselines
For GLUE, we train student models with 6 and 12
layers, 4 attention heads, and embedding/hidden
dimensions ﬁxed to 256, each using a compact 5K-
WP vocabulary. We also evaluate baselines with-
out knowledge distillation (NoKD), parameterized
identically to the distilled student models (incl. the
5K vocabulary), trained on the MLM teacher ob-
jective from scratch. We also compare our models
on GLUE with the following approaches:
• DistilBERT (Sanh et al., 2019) distill BERTBASE

to 4/6 layers by aligning teacher predictions,

• Patient KD - PKD (Sun et al., 2019) align hidden

states to distill BERTBASE to 3/6 layers,

• BERT-of-Theseus (Xu et al., 2020) use a layer
dropout method to distill BERTBASE to 6 layers,

Model

BERTBASE (Devlin et al., 2018)
BERTLARGE (Devlin et al., 2018)
PKD6 (Sun et al., 2019)
PKD3 (Sun et al., 2019)
DistilBERT4 (Sanh et al., 2019)
MobileBERT (Sun et al., 2020)
TinyBERT4 (Jiao et al., 2019)
† (Jiao et al., 2019)
TinyBERT4
BERT-of-Theseus6
NoKD Baseline, L-6, H-256
Mixed-vocab distilled (ours), L-6, H-256
NoKD Baseline, L-12, H-256
Mixed-vocab distilled (ours), L-12, H-256

† (Xu et al., 2020)

* denotes metrics on the development set

(Acc)

#Params MRPC MNLI-m/mm SST-2 Average
(Acc) (F1/Acc)
93.5
94.9
92.0
87.5
91.4
92.8

89.0
90.3
86.2
81.6
84.2
88.3

84.6/83.4
86.7/85.9
81.5/81.0
76.7/76.3
78.9/78.0
83.3/82.6
76.6/77.2*
82.5/81.8
82.4/82.1
76.9/76.1
79.0/78.6
79.1/79.0
80.7/80.5

-

92.6
92.2
87.0
89.1
89.4
90.6

-

87.2
87.4
81.7
84.3
84.5
86.2

82.4/-

(F1/Acc)
88.9/-
109M
340M
89.3/-
67.0M 85.0/79.9
45.7M 80.7/72.5
52.2M
25.3M 88.8/84.5
14.5M 82.0*/ -
14.5M
86.4/-
66M 87.6/83.2
81.2/74.1
84.9/79.3
85.1/79.8
87.2/82.6

10.9M

6.2M

† denotes results with task-speciﬁc distillation

Table 1: Test set accu-
racy of distilled models,
teacher model and base-
lines on the GLUE test
sets, along with other pa-
rameters. MNLI-m and
MNLI-mm refer
to the
genre-matched and mis-
matched test sets. All
models other than NoKD
and our distilled models
use a 30K-WordPiece vo-
cabulary. The average is
computed using F1 score
for MRPC and accuracy
for MNLI-m and SST-2.

• TinyBERT (Jiao et al., 2019) apply task speciﬁc
distillation to BERTBASE and align teacher out-
puts, hidden states as well as embeddings, and

• MobileBERT (Sun et al., 2020) combine layer-
wise transfer, architecture search and bottleneck
structures for an optimized student model.
For SNIPS, we shift our focus to smaller, low-
latency models for on-device use cases. Here,
we train student models with 6 layers and em-
bedding/hidden dimensions ∈{96, 192, 256}. The
smaller models here may not be competitive on
GLUE but are adequate for practical tasks such as
spoken LU. We compare with two strong baselines:
• BERTBASE (Chen et al., 2019a) with intent and
IOB slot tags predicted using the [CLS] and the
ﬁrst WP tokens of each word respectively, and

• StackProp (Qin et al., 2019), which uses a series
of smaller recurrent and self-attentive encoders.

4.3 Training Details
Distillation: For all our models, we train the
teacher model with mixed-vocabulary inputs (stage
I) for 500K steps, followed by 300K steps of train-
ing just the student model (stage II). We utilize the
same corpora as the teacher model i.e. BooksCor-
pus (Zhu et al., 2015) and English Wikipedia.

For both stages, up to 20 input tokens were
masked for MLM. In stage I, up to 10 of these
masked tokens were tokenized using the teacher
vocabulary, the rest using the student vocabulary.
We optimize the loss using LAMB (You et al.,
2019) with a max learning rate of .00125, linear
warmup for the ﬁrst 10% of steps, batch size of
2048 and sequence length of 128. Distillation was

done on Cloud TPUs in a 8x8 pod conﬁguration.
pSV , the probability of segmenting a Stage I input
word using the student vocabulary, is set to 0.5.
Finetuning: For all downstream task evaluations
on GLUE, we ﬁnetune for 10 epochs using LAMB
with a learning rate of 0.0001 and batch size of 64.
For all experiments on SNIPS, we use ADAM with
a learning rate of 0.0001 and a batch size of 64.

5 Results

GLUE: Table 1 shows results on the downstream
GLUE tasks alongside model sizes, for our models,
BERTBASE/LARGE, and all baselines.

Our proposed models consistently improve upon
the identically parameterized NoKD baselines, in-
dicating that mixed-vocabulary training is better
than training from scratch and avoids a drastic drop
in performance from the teacher. Compared with
PKD and DistilBERT, our 6-layer model outper-
forms PKD3 while being >7x smaller and our 12-
layer model performs competitively with PKD6
and DistilBERT4 while being ∼5-6x smaller.

Interestingly, our models do particularly well
the 6-layer distilled model
on the MRPC task:
performs almost as well as PKD6 while being over
10x smaller. This may be due to our smaller models
being data-efﬁcient on the smaller MRPC dataset.
TinyBERT and Bert-of-Theseus are trained in
task-speciﬁc fashion i.e., a teacher model already
ﬁnetuned on the downstream task is used for dis-
tillation. TinyBERT’s non-task-speciﬁc model re-
sults are reported on GLUE dev sets: these results
are, therefore, not directly comparable with ours.
Even so, our 12-layer model performs credibly

Model
BERTBASE (Chen et al., 2019a)
StackProp (Qin et al., 2019)
Mixed-vocab distilled, L-6, H-96
Mixed-vocab distilled, L-6, H-192
Mixed-vocab distilled, L-6, H-256

#Params Latency Intent Acc Slot F1
97.0
94.2
92.8
94.6
95.0

109M 340ms
2.6M >70ms
1.2M
3.6M
6.2M

98.6
98.0
98.9
98.8
98.7

6ms
14ms
20ms

Table 2: Results on the SNIPS dataset. Latency is mea-
sured with 4 CPU threads on a Pixel 4 mobile device.

compared with the two, presenting a competitive
size-accuracy tradeoff, particularly when compared
to the 6x larger BERT-of-Theseus.

MobileBERT performs strongly for the size
while being task-agnostic. Our 12-layer model,
in comparison, retains ∼98% of its performance
with 57% fewer parameters and may thus be better-
suited for use on highly resource-limited devices.
TinyBERT sees major gains from task-speciﬁc
data augmentation and distillation, and Mobile-
BERT from student architecture search and bot-
tleneck layers. Notably, our technique targets the
student vocabulary without conﬂicting with any of
the above methods and can, in fact, be combined
with these methods for even smaller models.
SNIPS: Table 2 shows results on the SNIPS intent
and slot tasks for our models and two state-of-the-
art baselines. Our smallest 6-layer model retains
over 95% of the BERTBASE model’s slot ﬁlling F1
score (Sang and Buchholz, 2000) while being 30x
smaller (< 10 MB w/o quantization) and 57x faster
on a mobile device, yet task-agnostic. Our other
larger distilled models also demonstrate strong per-
formance (0.2-0.5% slot F1 higher than the respec-
tive NoKD baselines) with small model sizes and
latencies low enough for real-time inference. This
indicates that small multi-task BERT models (Tsai
et al., 2019) present better trade-offs for on-device
usage for size, accuracy and latency versus recur-
rent encoder-based models such as StackProp.

6 Discussion
Impact of vocabulary size: We trained a model
from scratch identical to BERTBASE except with
our 5K-WP student vocabulary. On the SST-2 and
MNLI-m dev sets, this model obtained 90.9% and
83.7% accuracy respectively - only 1.8% and 0.7%
lower respectively compared to BERTBASE.

Since embeddings account for a larger fraction
of model parameters with fewer layers, we trained
another model identical to our 6×256 model, but
with a 30K-WP vocabulary and teacher label dis-

tillation. This model showed small gains (0.1%
/ 0.5% accuracy on SST-2 / MNLI-m dev) over
our analogous distilled model, but with 30% more
parameters solely due to the larger vocabulary.

This suggests that a small WordPiece vocabulary
may be almost as effective for sequence classiﬁ-
cation/tagging tasks, especially for smaller BERT
models and up to moderately long inputs. Curi-
ously, increasing the student vocabulary size to 7K
or 10K did not lead to an increase in performance
on GLUE. We surmise that this may be due to un-
derﬁtting owing to the embeddings accounting for
a larger proportion of the model parameters.
Alternative vocabulary pruning: Probing other
strategies for a small-vocabulary model, we used
the above 6×256 30K-WP vanilla distilled model
to obtain a smaller model by pruning the vocab-
ulary to contain the intersection of the 30K and
5K vocabularies (total 4629 WPs). This model is
1.2% smaller than our 4928-WP distilled model,
but drops 0.8% / 0.7% on SST-2/MNLI-m dev sets.
Furthermore, to exploit the high overlap in vo-
cabularies, we tried running our distillation pipeline
but with the embeddings for student tokens (after
projecting up to the teacher dimension) also present
in the teacher vocabulary tied to the teacher em-
beddings for those tokens. This model, however,
dropped 0.7% / 0.5% on SST-2/MNLI-m compared
to our analogous 6×256 distilled model.

We also tried pretraining BERTLARGE from
scratch with the 5K vocabulary and doing vanilla
distillation for a 6×256 student:
this model
dropped 1.2% / 0.7% for SST-2/MNLI-m over our
similar distilled model, indicating the efﬁcacy of
mixed-vocabulary training over vanilla distillation.

7 Conclusion

We propose a novel approach to knowledge distil-
lation for BERT, focusing on using a signiﬁcantly
smaller vocabulary for the student BERT models.
Our mixed-vocabulary training method encourages
implicit alignment of the teacher and student Word-
Piece embeddings. Our highly-compressed 6 and
12-layer distilled student models are optimized for
on-device use cases and demonstrate competitive
performance on both benchmark datasets and prac-
tical tasks. Our technique is unique in targeting the
student vocabulary size, enabling easy combination
with most BERT distillation methods.

References
Jimmy Ba and Rich Caruana. 2014. Do deep nets really
need to be deep? In Advances in neural information
processing systems, pages 2654–2662.

Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen
Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun
Huang, Wei Lin, and Jingren Zhou. 2020. Ad-
abert: Task-adaptive bert compression with differ-
entiable neural architecture search. arXiv preprint
arXiv:2001.04246.

Qian Chen, Zhu Zhuo, and Wen Wang. 2019a. Bert
for joint intent classiﬁcation and slot ﬁlling. arXiv
preprint arXiv:1902.10909.

Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng
Yan, and William Yang Wang. 2019b. How large
a vocabulary does text classiﬁcation need? a varia-
tional approach to vocabulary selection. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 3487–3497.

Wenlin Chen, James Wilson, Stephen Tyree, Kilian
Weinberger, and Yixin Chen. 2015. Compressing
In Inter-
neural networks with the hashing trick.
national Conference on Machine Learning, pages
2285–2294.

Alice Coucke, Alaa Saade, Adrien Ball, Th´eodore
Bluche, Alexandre Caulier, David Leroy, Cl´ement
Doumouro, Thibault Gisselbrecht, Francesco Calta-
girone, Thibaut Lavril, et al. 2018. Snips voice plat-
form: an embedded spoken language understanding
system for private-by-design voice interfaces. arXiv
preprint arXiv:1805.10190.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the International Workshop on
Paraphrasing.

Angela Fan, Edouard Grave, and Armand Joulin. 2019.
Reducing transformer depth on demand with struc-
tured dropout. arXiv preprint arXiv:1909.11556.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling bert for natural language
understanding. arXiv preprint arXiv:1909.10351.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
arXiv preprint
ing of language representations.
arXiv:1909.11942.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet,
and Hans Peter Graf. 2016. Pruning ﬁlters for efﬁ-
cient convnets. arXiv preprint arXiv:1608.08710.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference
on empirical methods in natural language process-
ing (EMNLP), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237.

Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita.
2019. Bpe-dropout: Simple and effective subword
regularization. arXiv preprint arXiv:1910.13267.

Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,
and Ting Liu. 2019. A stack-propagation frame-
work with token-level intent detection for spoken
In Proceedings of the
language understanding.
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 2078–2087.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2014. Fitnets: Hints for thin deep nets.
arXiv preprint arXiv:1412.6550.

Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task chunk-
In Fourth Conference on Computational Nat-
ing.
ural Language Learning and the Second Learning
Language in Logic Workshop.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and
Kurt Keutzer. 2019. Q-bert: Hessian based ultra
low precision quantization of bert. arXiv preprint
arXiv:1909.05840.

Raphael Shu and Hideki Nakayama. 2018. Compress-
ing word embeddings via deep compositional code
learning. In International Conference on Learning
Representations.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. The journal of machine learning
research, 15(1):1929–1958.

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for bert model com-
pression. arXiv preprint arXiv:1908.09355.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. Mobilebert:
a compact task-agnostic bert for resource-limited de-
vices. arXiv preprint arXiv:2004.02984.

Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga
Vechtomova, and Jimmy Lin. 2019. Distilling task-
speciﬁc knowledge from bert into simple neural net-
works. arXiv preprint arXiv:1903.12136.

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-
vazhagan, Xin Li, and Amelia Archer. 2019. Small
and practical bert models for sequence labeling. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3623–
3627.

Andrew Tulloch and Yangqing Jia. 2017. High perfor-
mance ultra-low-precision convolutions on mobile
devices. arXiv preprint arXiv:1712.02427.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
In Inter-
form for natural language understanding.
national Conference on Learning Representations.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
arXiv preprint
man and machine translation.
arXiv:1609.08144.

Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and
Jimmy Lin. 2020. Deebert: Dynamic early exit-
ing for accelerating bert inference. arXiv preprint
arXiv:2004.12993.

Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,
and Ming Zhou. 2020. Bert-of-theseus: Compress-
ing bert by progressive module replacing. arXiv
preprint arXiv:2002.02925.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
arXiv preprint
ing for language understanding.
arXiv:1906.08237.

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu,
Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,
James Demmel, and Cho-Jui Hsieh. 2019. Large
batch optimization for deep learning: Training bert
in 76 minutes. arXiv preprint arXiv:1904.00962.

Sergey Zagoruyko and Nikos Komodakis. 2016. Pay-
ing more attention to attention: Improving the per-
formance of convolutional neural networks via atten-
tion transfer. arXiv preprint arXiv:1612.03928.

Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-
Man Cheung, and Pascal Frossard. 2018. Adap-
tive quantization for deep neural network. In Thirty-
Second AAAI Conference on Artiﬁcial Intelligence.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE inter-
national conference on computer vision, pages 19–
27.

